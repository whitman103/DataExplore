\documentclass{article}
\usepackage{amsmath,amsfonts,amsthm,graphicx,tabularx,grffile,epstopdf,esint,physics,bbm,tabu,amstext,setspace,tikz,framed,adjustbox,array,subcaption,pgf,rotating,float,multirow,multicol,indentfirst,enumitem}
\usepackage[table]{colortbl}
\usepackage[compact]{titlesec}
\usepackage[top=1 in, left=.9 in, right=.9 in, bottom=1 in]{geometry}
\usetikzlibrary{arrows,decorations.pathmorphing,shapes,positioning}
\newenvironment{question}{\fontfamily{pnc}\selectfont \large}{\par}
\newcommand{\reaction}[1]{\overset{#1}{\rightarrow}}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\dualreaction}[2]{\underset{#1}{\overset{#2}{\rightleftharpoons}}}
\newcommand{\D}[2]{\frac{d #1}{d #2}}
\newcommand{\pline}[0]{\par \noindent \rule{.5\linewidth}{1pt}\par}
\DeclareMathOperator{\EX}{\mathbb{E}}
\newcommand{\inv}[0]{$^{-1}$}
\newcommand{\avg}[1]{\langle #1\rangle}
\newcommand{\ninfty}[0]{{-\infty}}
\newcolumntype{L}{>{$}l<{$}} % math-mode version of "l" column type
\newcolumntype{C}{>{$}c<{$}}
\newcommand{\custitle}[2]{\begin{center}\Large \textbf{#1}\vspace{.5cm}\par\large\textit{#2}\par\small \today\end{center}}
\newcommand{\parbreak}[0]{\par \vspace{0.5cm}}
\newcommand{\leader}[1]{\textbf{\textit{#1:}}}

\usepackage{lineno,hyperref,tikz}
\modulolinenumbers[5]
\graphicspath{{texfigures//}}
\begin{document}
\onehalfspacing
\custitle{Plan for TissueFBA}{John Whitman}
\leader{Purpose} Establish the idea of the PCA cloud. 
\section*{Math}
We have a matrix, $\mathbf{D}$, with dimension $m\times n$, where $m$ is the number of observations of data, and $n$ is the number of features collected per sample. We define the PCA function as taking in a matrix of some dimension, and returning a matrix $\mathbf{B}$ of dimension $o \times n$ where $o$ is the desired, reduced number of dimensions of the operation. Given a data vector $\vec{d}$ (a row from $\mathbf{D})$ which is dimension $1 \times n$ by definition, we can transform the point into the target space by:
\begin{equation}
    \vec{d}' = \vec{d} \times \mathbf{B}^T
\end{equation}
yielding a new vector of $1 \times o$ dimension.\par
Our goal is to measure the variability in dimensional reduction as a function of subsampling the data population. To do this, we first generate an eigenbasis using the full dataset, $\mathbf{D}$. We define this eigenbasis as $\mathbf{B}_0$. We also generate the transformed datapoints in the eigenbasis: $\{\vec{d_i}'_0 = {\vec{d_i}\times \mathbf{B}_0^T :  i \in \{1,\dots, m\},\ d_i \in \mathbf{D}}\}$. For absolute clarity, $\vec{d_i}'_0$ is the transformed datapoint relating to row $i$ of the original datamatrix, in the reduced dimensionality eigenbasis generated by the PCA of the full dataset.\par
Now, we generate a new datamatrix through subsampling out the population; this datamatrix is of size $l \times n$ where $l\leq m$ is the size of the subsample. Running PCA on this datamatrix (keeping sure to maintain the same target rank of the output dimensional reduction) generates a new eigenbasis $\mathbf{B}'$, with which we may generate new transformed datapoints: $\{\vec{\hat{e}}_j = \vec{e_j} \times \mathbf{B}'^T : j \in \{1,\dots, l\}, \ e_j \in \mathbf{D}'\}$.\par
At this point, we have up to three representations of any particular data point $i$: the original generation in full feature space ($\vec{d}_i$), the dimensional reduced point in $\mathbf{B}$ ($\vec{d}_i'$), and, if the data point was in the subsampled population of $\mathbf{D}'$, another dimensionally reduced point ($\vec{e}_i'$). We note that we can project $\vec{\hat{e}}_i'$ into the space of $\mathbf{B}$, since both $\mathbf{B}$ and $\mathbf{B}'$ will be orthogonal eigenbases. In plain terms, this projection provides an abstract measure of the variation in the original PCA as a function of changing the subpopulation of the projection.\par
We quickly calculate the transformation by moving $\vec{\hat{e}}_i'$ back into feature space and then into $\mathbf{B}$. 
\begin{equation}
\begin{aligned}
    \vec{e}_i &= \vec{\hat{e}}_i\times \left(\mathbf{B}'^T\right)^{-1}\\
    \vec{e}'_i & = \vec{\hat{e}}_i \times \left(\mathbf{B}'^T\right)^{-1} \times B^T
\end{aligned}
\end{equation}
where we imply, since $\mathbf{B}'$ is generally non-square, that $\mathbf{B}'^{-1}$ is the Monroe-Penrose psuedo-inverse. Interestingly, since $\mathbf{B}'$ is, by construction, an orthogonal matrix, the matrix inverse is just the transpose. So this matrix multiplication reduces to just $\mathbf{B}' \times B^T$. As a gut check, if $\mathbf{B}'=\mathbf{B}$, this would be the identity matrix and there would be no transformation. 


\end{document}